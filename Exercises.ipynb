{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running it in Google Colab, please run this first\n",
    "# !wget https://raw.githubusercontent.com/dvgodoy/PyTorch101_AI_Plus/main/v4.py -O v4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "# url = 'https://raw.githubusercontent.com/dvgodoy/PyTorch101_AI_Plus/main/mpg/auto-mpg.data'\n",
    "column_names = ['mpg', 'cyl', 'disp', 'hp', 'weight', 'acc', 'year', 'origin']\n",
    "\n",
    "df = pd.read_csv(url, names=column_names, na_values='?', comment='\\t', sep=' ', skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ...\n",
    "y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ...\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "x_train_tensor = ...\n",
    "y_train_tensor = ...\n",
    "\n",
    "x_val_tensor = ...\n",
    "y_val_tensor = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "b = ...\n",
    "w = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads a script into Colab\n",
    "#!curl https://raw.githubusercontent.com/dvgodoy/PyTorch101_AI_Plus/main/gradient_descent.py --output gradient_descent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import iplot, init_notebook_mode\n",
    "from ipywidgets import VBox, IntSlider, FloatSlider, Dropdown\n",
    "from gradient_descent import *\n",
    "import ipywidgets as widgets\n",
    "\n",
    "init_notebook_mode(connected=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = FloatSlider(description='Start', value=-1.5, min=-2, max=2, step=.05)\n",
    "functype = Dropdown(description='Function', options=['Convex', 'Non-Convex'], value='Convex')\n",
    "lrate = FloatSlider(description='Learning Rate', value=.05, min=.05, max=1.1, step=.05)\n",
    "n_steps = IntSlider(description='# updates', value=10, min=10, max=20, step=1)\n",
    "\n",
    "def f(functype, lrate, w0, n_steps):\n",
    "    fig = build_fig(functype, lrate, w0, n_steps)\n",
    "    display(iplot(fig))\n",
    "\n",
    "configure_plotly_browser_state()\n",
    "out = widgets.interactive_output(f, {'functype': functype, 'lrate': lrate, 'w0': w0, 'n_steps': n_steps})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with Learning Rates\n",
    "\n",
    "Let's work through **an interactive example**!\n",
    "\n",
    "We start at a (not so) **random initial value** of our **feature**, say, -1.5. It has a corresponding **loss** of 2.25.\n",
    "\n",
    "You can choose between **two functions**:\n",
    "- **convex**, meaning, its **loss is well-behaved** and **gradient descent is guaranteed to converge**\n",
    "- **non-convex**, meaning, **all bets are off**!\n",
    "\n",
    "Every time you **take a step**, the plot gets updated:\n",
    "\n",
    "- The **red vector** is our update to the **weight**, that is, **learning rate times gradient**.\n",
    "\n",
    "- The **gray vecto**r shows **how much the cost changes** given our update.\n",
    "\n",
    "- If you divide their lengths, **gray over red**, it will give you the **approximate gradient**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_plotly_browser_state()\n",
    "VBox((w0, functype, lrate, n_steps, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "\n",
    "1. Choose a different learning rate, reset the plot and follow some steps. Observe the path it traces and check if it hits the minimum. Try different learning rates, see what happens if you choose a really big value for it.\n",
    "\n",
    "\n",
    "2. Then, change the function to a ***Non-convex*** and set the learning rate to the minimum before following some steps. Where does it converge to? Try resetting and observing its path. Does it reach the global minimum? Try different learning rates and see what happens then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl https://raw.githubusercontent.com/dvgodoy/PyTorch101_AI_Plus/main/scaling.py --output scaling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import iplot, init_notebook_mode\n",
    "from ipywidgets import VBox, IntSlider, FloatSlider, Dropdown\n",
    "from scaling import *\n",
    "import ipywidgets as widgets\n",
    "\n",
    "init_notebook_mode(connected=False)\n",
    "\n",
    "x1, x2, y = data()\n",
    "mygd = plotGradientDescent(x1, x2, y)\n",
    "fig, update, (lr, scaled, epochs, batch_size, m1, m2) = build_figure(mygd)\n",
    "\n",
    "def f(lr, scaled, epochs, batch_size, m1, m2):\n",
    "    update(lr, scaled, epochs, batch_size, m1, m2)\n",
    "    display(iplot(fig))\n",
    "\n",
    "configure_plotly_browser_state()\n",
    "out = widgets.interactive_output(f, {'lr': lr, 'scaled': scaled, 'epochs': epochs, 'batch_size': batch_size, 'm1': m1, 'm2': m2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #2.2\n",
    "\n",
    "There are two parameters, x1 and x2, and we're using Gradient Descent to try to reach the ***minimum*** indicated by the ***star***.\n",
    "\n",
    "The dataset has only 50 data points.\n",
    "\n",
    "The controls below allow you to:\n",
    "- adjust the learning rate\n",
    "- scale the features x1 and x2\n",
    "- set the number of epochs (steps)\n",
    "- batch size (since the dataset has 50 points, a size of 64 means using ***all*** points)\n",
    "- starting point for x1 and x2 (initialization)\n",
    "\n",
    "Use the controls to play with different configurations and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_plotly_browser_state()\n",
    "VBox((lr, scaled, epochs, batch_size, m1, m2, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1. ***Without scaling features***, start with the ***learning rate at minimum***:\n",
    "    - change the batch size - try ***stochastic***, ***batch*** and ***mini-batch*** sizes - what happens to the trajectory? Why?\n",
    "    - keeping ***maximum batch size***, increase ***learning rate*** to 0.000562 (three notches) - what happens to the trajectory? Why?\n",
    "    - now reduce gradually ***batch size*** - what happens to the trajectory? Why?\n",
    "    - go back to ***maximum batch size*** and, this time, increase ***learning rate*** a bit further- what happens to the trajectory? Why?\n",
    "    - experiment with different settings (yet ***no scaling***), including initial values ***x1*** and ***x2*** and try to get as close as possible to the ***minimum*** - how hard is it?\n",
    "    - what was the ***largest learning rate*** you manage to use succesfully?\n",
    "\n",
    "\n",
    "2. Check ***Scale Features*** - what happened to the surface (cost)? What about its level (look at the scale)?\n",
    "\n",
    "\n",
    "3. ***Using scaled features***, answer the same items as in ***question 1***.\n",
    "\n",
    "\n",
    "4. How do you compare the ***performance*** of gradient descent with and without ***scaling***? Why did this happen? (think about the partial derivatives with respect to each feature, especially without scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling / Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "x_train_sc = ...\n",
    "x_val_sc = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines number of epochs\n",
    "n_epochs = 100\n",
    "lr = ...\n",
    "\n",
    "# Step 0\n",
    "np.random.seed(42)\n",
    "...\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1\n",
    "    yhat = ...\n",
    "    \n",
    "    # Step 2\n",
    "    error = ...\n",
    "    loss = ...\n",
    "\n",
    "    # Step 3    \n",
    "    b_grad = ...\n",
    "    w_grad = ...\n",
    "    \n",
    "    # Step 4\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linr = LinearRegression()\n",
    "linr.fit(x_train_sc, y_train)\n",
    "print(linr.intercept_, linr.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = ...\n",
    "y_train_tensor = ...\n",
    "\n",
    "x_val_tensor = ...\n",
    "y_val_tensor = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = ...\n",
    "n_epochs = 100\n",
    "\n",
    "# Step 0\n",
    "torch.manual_seed(42)\n",
    "b = ...\n",
    "w = ...\n",
    "\n",
    "loss_fn = ...\n",
    "optimizer = ...\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1\n",
    "    yhat = ...\n",
    "    \n",
    "    # Step 2\n",
    "    loss = ...\n",
    "\n",
    "    # Step 3\n",
    "    ...\n",
    "\n",
    "    # Step 4\n",
    "    ...\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step_fn(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def perform_train_step_fn(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        \n",
    "        # Step 1\n",
    "        yhat = ...\n",
    "        # Step 2\n",
    "        loss = ...\n",
    "        # Step 3\n",
    "        ...\n",
    "        # Step 4\n",
    "        ...\n",
    "        \n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    # Returns the function that will be called inside the train loop\n",
    "    return perform_train_step_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "model = ...\n",
    "loss_fn = ...\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_fn = ...\n",
    "\n",
    "n_epochs = ...\n",
    "\n",
    "losses = []\n",
    "# For each epoch...\n",
    "for epoch in range(n_epochs):\n",
    "    # Performs one train step and returns the corresponding loss\n",
    "    loss = ...\n",
    "    losses.append(loss)\n",
    "    \n",
    "# Checks model's parameters\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_val_step_fn(model, loss_fn):\n",
    "    # Builds function that performs a step in the validation loop\n",
    "    def perform_val_step_fn(x, y):\n",
    "        # Sets model to EVAL mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Step 1\n",
    "        yhat = ...\n",
    "        # Step 2\n",
    "        loss = ...\n",
    "\n",
    "        return loss.item()\n",
    "    \n",
    "    return perform_val_step_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ...\n",
    "val_dataset = ...\n",
    "\n",
    "# builds a loader of each set\n",
    "train_loader = ...\n",
    "val_loader = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "model = ...\n",
    "loss_fn = ...\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_fn = ...\n",
    "\n",
    "val_step_fn = ...\n",
    "\n",
    "n_epochs = ...\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    mini_batch_losses = []\n",
    "    # inner loop for mini batches\n",
    "    ...\n",
    "\n",
    "    loss = np.mean(mini_batch_losses)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # VALIDATION\n",
    "    # no gradients in validation!\n",
    "    with torch.no_grad():\n",
    "        mini_batch_losses = []\n",
    "        ...\n",
    "\n",
    "        val_loss = np.mean(mini_batch_losses)\n",
    "        val_losses.append(val_loss) \n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StepByStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "model = ...\n",
    "loss_fn = ...\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from v4 import StepByStep\n",
    "sbs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sbs.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_train = sbs.predict(x_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.scatter(y_train, yhat_train)\n",
    "ax.plot([0, 45], [0, 45], linestyle='--', c='k', linewidth=1)\n",
    "ax.set_xlabel('True')\n",
    "ax.set_xlim([0, 45])\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.set_ylim([0, 45])\n",
    "ax.set_title('MPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_train, yhat_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seven Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seven = df.loc[:, ['mpg', 'cyl', 'disp', 'hp', 'weight', 'acc', 'year']].dropna()\n",
    "\n",
    "x = df_seven.loc[:, ['cyl', 'disp', 'hp', 'weight', 'acc', 'year']].values\n",
    "y = df_seven.loc[:, ['mpg']].values\n",
    "\n",
    "x_train, x_val, y_train, y_val = ...\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train_sc = ...\n",
    "x_val_sc = ...\n",
    "\n",
    "x_train_tensor = ...\n",
    "y_train_tensor = ...\n",
    "\n",
    "x_val_tensor = ...\n",
    "y_val_tensor = ...\n",
    "\n",
    "# builds datasets\n",
    "train_dataset = ...\n",
    "val_dataset = ...\n",
    "\n",
    "# builds a loader of each set\n",
    "train_loader = ...\n",
    "val_loader = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "model = ...\n",
    "loss_fn = ...\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sbs.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_train = sbs.predict(x_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_train, yhat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_train, yhat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
