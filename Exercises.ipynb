{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running it in Google Colab, please run this first\n",
    "# !wget https://raw.githubusercontent.com/dvgodoy/PyTorch101_AI_Plus/main/v4.py -O v4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #1\n",
    "\n",
    "We'll be using UCI's [Auto MPG Dataset](https://archive.ics.uci.edu/ml/datasets/auto+mpg) to trying predict the consumption (miles per gallon, MPG) based on the car's characteristics. Initially, we'll focus on a single feature, horsepower (HP), thus making it a simple linear regression with a single feature.\n",
    "\n",
    "These are the steps you need to implement:\n",
    "- create a DF containing only the feature and the target/label\n",
    "  - drop any entries with missing data to keep the example simple\n",
    "- assign the feature to a Numpy array (x) and the target/label to another (y)\n",
    "- perform a train-validation split\n",
    "- create PyTorch tensors for features and targets/labels\n",
    "  - make sure you create the tensors in the right device (CUDA, if available)\n",
    "- create tensors for the parameters (one for the bias/intercept, another for the one feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "# url = 'https://raw.githubusercontent.com/dvgodoy/PyTorch101_AI_Plus/main/mpg/auto-mpg.data'\n",
    "column_names = ['mpg', 'cyl', 'disp', 'hp', 'weight', 'acc', 'year', 'origin']\n",
    "\n",
    "df = pd.read_csv(url, names=column_names, na_values='?', comment='\\t', sep=' ', skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ...\n",
    "y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ...\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "x_train_tensor = ...\n",
    "y_train_tensor = ...\n",
    "\n",
    "x_val_tensor = ...\n",
    "y_val_tensor = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "b = ...\n",
    "w = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads a script into Colab\n",
    "#!curl https://raw.githubusercontent.com/dvgodoy/PyTorch101_AI_Plus/main/gradient_descent.py --output gradient_descent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import iplot, init_notebook_mode\n",
    "from ipywidgets import VBox, IntSlider, FloatSlider, Dropdown\n",
    "from gradient_descent import *\n",
    "import ipywidgets as widgets\n",
    "\n",
    "init_notebook_mode(connected=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = FloatSlider(description='Start', value=-1.5, min=-2, max=2, step=.05)\n",
    "functype = Dropdown(description='Function', options=['Convex', 'Non-Convex'], value='Convex')\n",
    "lrate = FloatSlider(description='Learning Rate', value=.05, min=.05, max=1.1, step=.05)\n",
    "n_steps = IntSlider(description='# updates', value=10, min=10, max=20, step=1)\n",
    "\n",
    "def f(functype, lrate, w0, n_steps):\n",
    "    fig = build_fig(functype, lrate, w0, n_steps)\n",
    "    display(iplot(fig))\n",
    "\n",
    "configure_plotly_browser_state()\n",
    "out = widgets.interactive_output(f, {'functype': functype, 'lrate': lrate, 'w0': w0, 'n_steps': n_steps})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with Learning Rates\n",
    "\n",
    "Let's work through **an interactive example**!\n",
    "\n",
    "We start at a (not so) **random initial value** of our **feature**, say, -1.5. It has a corresponding **loss** of 2.25.\n",
    "\n",
    "You can choose between **two functions**:\n",
    "- **convex**, meaning, its **loss is well-behaved** and **gradient descent is guaranteed to converge**\n",
    "- **non-convex**, meaning, **all bets are off**!\n",
    "\n",
    "Every time you **take a step**, the plot gets updated:\n",
    "\n",
    "- The **red vector** is our update to the **weight**, that is, **learning rate times gradient**.\n",
    "\n",
    "- The **gray vecto**r shows **how much the cost changes** given our update.\n",
    "\n",
    "- If you divide their lengths, **gray over red**, it will give you the **approximate gradient**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_plotly_browser_state()\n",
    "VBox((w0, functype, lrate, n_steps, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "\n",
    "1. Choose a different learning rate, reset the plot and follow some steps. Observe the path it traces and check if it hits the minimum. Try different learning rates, see what happens if you choose a really big value for it.\n",
    "\n",
    "\n",
    "2. Then, change the function to a ***Non-convex*** and set the learning rate to the minimum before following some steps. Where does it converge to? Try resetting and observing its path. Does it reach the global minimum? Try different learning rates and see what happens then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl https://raw.githubusercontent.com/dvgodoy/PyTorch101_AI_Plus/main/scaling.py --output scaling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import iplot, init_notebook_mode\n",
    "from ipywidgets import VBox, IntSlider, FloatSlider, Dropdown\n",
    "from scaling import *\n",
    "import ipywidgets as widgets\n",
    "\n",
    "init_notebook_mode(connected=False)\n",
    "\n",
    "x1, x2, y = data()\n",
    "mygd = plotGradientDescent(x1, x2, y)\n",
    "fig, update, (lr, scaled, epochs, batch_size, m1, m2) = build_figure(mygd)\n",
    "\n",
    "def f(lr, scaled, epochs, batch_size, m1, m2):\n",
    "    update(lr, scaled, epochs, batch_size, m1, m2)\n",
    "    display(iplot(fig))\n",
    "\n",
    "configure_plotly_browser_state()\n",
    "out = widgets.interactive_output(f, {'lr': lr, 'scaled': scaled, 'epochs': epochs, 'batch_size': batch_size, 'm1': m1, 'm2': m2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #2.2\n",
    "\n",
    "There are two parameters, x1 and x2, and we're using Gradient Descent to try to reach the ***minimum*** indicated by the ***star***.\n",
    "\n",
    "The dataset has only 50 data points.\n",
    "\n",
    "The controls below allow you to:\n",
    "- adjust the learning rate\n",
    "- scale the features x1 and x2\n",
    "- set the number of epochs (steps)\n",
    "- batch size (since the dataset has 50 points, a size of 64 means using ***all*** points)\n",
    "- starting point for x1 and x2 (initialization)\n",
    "\n",
    "Use the controls to play with different configurations and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_plotly_browser_state()\n",
    "VBox((lr, scaled, epochs, batch_size, m1, m2, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1. ***Without scaling features***, start with the ***learning rate at minimum***:\n",
    "    - change the batch size - try ***stochastic***, ***batch*** and ***mini-batch*** sizes - what happens to the trajectory? Why?\n",
    "    - keeping ***maximum batch size***, increase ***learning rate*** to 0.000562 (three notches) - what happens to the trajectory? Why?\n",
    "    - now reduce gradually ***batch size*** - what happens to the trajectory? Why?\n",
    "    - go back to ***maximum batch size*** and, this time, increase ***learning rate*** a bit further- what happens to the trajectory? Why?\n",
    "    - experiment with different settings (yet ***no scaling***), including initial values ***x1*** and ***x2*** and try to get as close as possible to the ***minimum*** - how hard is it?\n",
    "    - what was the ***largest learning rate*** you manage to use succesfully?\n",
    "\n",
    "\n",
    "2. Check ***Scale Features*** - what happened to the surface (cost)? What about its level (look at the scale)?\n",
    "\n",
    "\n",
    "3. ***Using scaled features***, answer the same items as in ***question 1***.\n",
    "\n",
    "\n",
    "4. How do you compare the ***performance*** of gradient descent with and without ***scaling***? Why did this happen? (think about the partial derivatives with respect to each feature, especially without scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #2.3\n",
    "\n",
    "In this exercise, we'll standardize our feature (HP), and implement a training loop in Numpy, containing all the steps:\n",
    "- Step 0: creating/initializing the parameters\n",
    "- Step 1: making predictions using the parameters (the forward pass)\n",
    "- Step 2: computing the errors and the loss\n",
    "- Step 3: computing the gradients (this one is done for you!)\n",
    "- Step 4: updating the parameters\n",
    "\n",
    "To make sure our implementation is fine, we should also perform a sanity check, fitting a linear regression using Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling / Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "x_train_sc = ...\n",
    "x_val_sc = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines number of epochs\n",
    "n_epochs = 100\n",
    "lr = ...\n",
    "\n",
    "# Step 0\n",
    "np.random.seed(42)\n",
    "b = ...\n",
    "w = ...\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1\n",
    "    yhat = ...\n",
    "    \n",
    "    # Step 2\n",
    "    error = ...\n",
    "    loss = ...\n",
    "\n",
    "    # Step 3    \n",
    "    b_grad = 2 * error.mean()\n",
    "    w_grad = 2 * (x_train_sc * error).mean()\n",
    "    \n",
    "    # Step 4\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linr = LinearRegression()\n",
    "linr.fit(x_train_sc, y_train)\n",
    "print(linr.intercept_, linr.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #3\n",
    "\n",
    "We're back to PyTorch now, and we need to create tensors using the **scaled** feature from the previous exercise, and then (re)implement the training loop taking advantage of PyTorch's **loss functions** and **optimizers**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = ...\n",
    "y_train_tensor = ...\n",
    "\n",
    "x_val_tensor = ...\n",
    "y_val_tensor = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = ...\n",
    "n_epochs = 100\n",
    "\n",
    "# Step 0\n",
    "torch.manual_seed(42)\n",
    "b_tensor = ...\n",
    "w_tensor = ...\n",
    "\n",
    "loss_fn = ...\n",
    "optimizer = ...\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1\n",
    "    yhat = ...\n",
    "    \n",
    "    # Step 2\n",
    "    loss = ...\n",
    "\n",
    "    # Step 3\n",
    "    ...\n",
    "\n",
    "    # Step 4\n",
    "    ...\n",
    "    \n",
    "print(b_tensor, w_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #4\n",
    "\n",
    "We'll keep improving the training loop. \n",
    "\n",
    "First, we must encapsulate all the steps performed in a **training step** in a higher-order function (the scaffolding is already done).\n",
    "\n",
    "Then, we need to create the three basic elements for training a model (the \"Model Configuration\" part):\n",
    "- the model itself\n",
    "- a loss function\n",
    "- an optimizer\n",
    "\n",
    "Next, we use those elements to create an actual training step function, and use it inside a training loop (the \"Model Training\" part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step_fn(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def perform_train_step_fn(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        \n",
    "        # Step 1\n",
    "        yhat = ...\n",
    "        # Step 2\n",
    "        loss = ...\n",
    "        # Step 3\n",
    "        ...\n",
    "        # Step 4\n",
    "        ...\n",
    "        \n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    # Returns the function that will be called inside the train loop\n",
    "    return perform_train_step_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "model = ...\n",
    "loss_fn = ...\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_fn = ...\n",
    "\n",
    "n_epochs = ...\n",
    "\n",
    "losses = []\n",
    "# For each epoch...\n",
    "for epoch in range(n_epochs):\n",
    "    # Performs one train step and returns the corresponding loss\n",
    "    loss = ...\n",
    "    losses.append(loss)\n",
    "    \n",
    "# Checks model's parameters\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #5\n",
    "\n",
    "Now we'll incorporate **datasets**, **data loaders** and **validation** into our pipeline.\n",
    "\n",
    "First, we need to build yet another higher-order function that returns a function for the validation step.\n",
    "\n",
    "Then, we should create two datasets, one for training and one for validation, using the tensors we already have. Each dataset will feed its own data loader. That's the \"Data Preparation\" part.\n",
    "\n",
    "The \"Model Configuration\" part should be the same as in the previous exercise.\n",
    "\n",
    "The \"Model Training\" part needs to be modified to incorporate the validation. The context manager, `no_grad`, is already available in the scaffolding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_val_step_fn(model, loss_fn):\n",
    "    # Builds function that performs a step in the validation loop\n",
    "    def perform_val_step_fn(x, y):\n",
    "        # Sets model to EVAL mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Step 1\n",
    "        yhat = ...\n",
    "        # Step 2\n",
    "        loss = ...\n",
    "\n",
    "        return loss.item()\n",
    "    \n",
    "    return perform_val_step_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ...\n",
    "val_dataset = ...\n",
    "\n",
    "# builds a loader of each set\n",
    "train_loader = ...\n",
    "val_loader = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "model = ...\n",
    "loss_fn = ...\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_fn = ...\n",
    "\n",
    "val_step_fn = ...\n",
    "\n",
    "n_epochs = ...\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    mini_batch_losses = []\n",
    "    # inner loop for mini batches\n",
    "    ...\n",
    "\n",
    "    loss = np.mean(mini_batch_losses)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # VALIDATION\n",
    "    # no gradients in validation!\n",
    "    with torch.no_grad():\n",
    "        mini_batch_losses = []\n",
    "        ...\n",
    "\n",
    "        val_loss = np.mean(mini_batch_losses)\n",
    "        val_losses.append(val_loss) \n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StepByStep\n",
    "\n",
    "The StepByStep class handles the boilerplate for you. It is a \"toy framework\" of sorts, but it shows us what it is like to use other frameworks like PyTorch Lightning, fastai, or Ignite, for example.\n",
    "\n",
    "We need to create the elements of the \"Model Configuration\" part, namely, the model itself, a loss function, and an optimizer, and then use them to instantiate the StepByStep class.\n",
    "\n",
    "Next, we can set the loaders, and quickly train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "model = ...\n",
    "loss_fn = ...\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from v4 import StepByStep\n",
    "sbs = ...\n",
    "sbs.set_loaders(train_loader, val_loader)\n",
    "sbs.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sbs.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_train = sbs.predict(x_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.scatter(y_train, yhat_train)\n",
    "ax.plot([0, 45], [0, 45], linestyle='--', c='k', linewidth=1)\n",
    "ax.set_xlabel('True')\n",
    "ax.set_xlim([0, 45])\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.set_ylim([0, 45])\n",
    "ax.set_title('MPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_train, yhat_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Six Features\n",
    "\n",
    "Now, instead of using only one feature (HP), let's use six of them (CYL, DISP, HP, WEIGHT, ACC, and YEAR). We're not using categorical features here to make the pipeline simpler.\n",
    "\n",
    "Most of the changes, in this case, are in the \"Data Preparation\" part.\n",
    "\n",
    "We also need to change the model, since there are more features now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_six = df.loc[:, ['mpg', 'cyl', 'disp', 'hp', 'weight', 'acc', 'year']].dropna()\n",
    "\n",
    "x = df_six.loc[:, ['cyl', 'disp', 'hp', 'weight', 'acc', 'year']].values\n",
    "y = df_six.loc[:, ['mpg']].values\n",
    "\n",
    "x_train, x_val, y_train, y_val = ...\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train_sc = ...\n",
    "x_val_sc = ...\n",
    "\n",
    "x_train_tensor = ...\n",
    "y_train_tensor = ...\n",
    "\n",
    "x_val_tensor = ...\n",
    "y_val_tensor = ...\n",
    "\n",
    "# builds datasets\n",
    "train_dataset = ...\n",
    "val_dataset = ...\n",
    "\n",
    "# builds a loader of each set\n",
    "train_loader = ...\n",
    "val_loader = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "model = ...\n",
    "loss_fn = ...\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sbs.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_train = sbs.predict(x_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_train, yhat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_train, yhat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
