{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }.text_cell_render, .output_text {font-family: Lato;font-size: 18px;line-height: 1.5;}.CodeMirror {font-size: 16px;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"<style>.container { width:90% !important; }.text_cell_render, .output_text {font-family: Lato;font-size: 18px;line-height: 1.5;}.CodeMirror {font-size: 16px;}</style>\"\"\"))\n",
    "from quiz.jupyterquiz import display_quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Fh8LOI0yYT7"
   },
   "source": [
    "## Model: making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A03THT7YybBn"
   },
   "source": [
    "In PyTorch, a **model** is represented by a regular **Python class** that inherits from the [**Module**](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) class.\n",
    "\n",
    "The most fundamental methods it needs to implement are:\n",
    "\n",
    "* **`__init__(self)`**: **it defines the parts that make up the model** —in our case, two parameters, **b** and **w**.\n",
    "\n",
    "* **`forward(self, x)`**: it performs the **actual computation**, that is, it **outputs a prediction**, given the input **x**.\n",
    "\n",
    "Let’s build a proper (yet simple) model for our regression task. It should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SNMj2ScHyoXU"
   },
   "outputs": [],
   "source": [
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "        w = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "\n",
    "        # To make \"b\" and \"w\" real parameters of the model, we need to wrap them with nn.Parameter\n",
    "        self.b = nn.Parameter(b)\n",
    "        self.w = nn.Parameter(w)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.b + self.w * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4CN2oGK0G7z"
   },
   "source": [
    "### Parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FBc6HURx0F2i"
   },
   "source": [
    "In the **\\__init__** method, we define our **two parameters**, **b** and **w**, using the [**Parameter()**](https://bit.ly/309iFQ6) class, to tell PyTorch these **tensors should be considered parameters of the model they are an attribute of**.\n",
    "\n",
    "Why should we care about that? By doing so, we can use our model’s [**parameters()**](https://bit.ly/3jT0Hte[) method to retrieve **an iterator over all model’s parameters**, even those parameters of **nested models**, that we can use to feed our optimizer (instead of building a list of parameters ourselves!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "Nwf_kZPhz0Qw",
    "outputId": "902fcc06-2fb3-452c-c929-f1303080a894"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1288], requires_grad=True)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = ManualLinearRegression()\n",
    "\n",
    "list(dummy.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K-5BiAQs08Tn"
   },
   "source": [
    "Moreover, we can get the **current values for all parameters** using our model’s [**state_dict()**](https://bit.ly/3f8mnOs) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "htdYIsSD0q2G",
    "outputId": "5ce48458-e93d-414f-d75f-cd7d4989da5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('b', tensor([0.3367])), ('w', tensor([0.1288]))])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIWl41iKdJdP"
   },
   "source": [
    "### state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WU5er3Y4MdO0"
   },
   "source": [
    "The **state_dict()** of a given model is simply a Python dictionary that **maps each layer / parameter to its corresponding tensor**. But only **learnable** parameters are included, as its purpose is to keep track of parameters that are going to be updated by the **optimizer**.\n",
    "\n",
    "The **optimizer** itself also has a **state_dict()**, which contains its internal state, as well as the hyperparameters used.\n",
    "\n",
    "---\n",
    "\n",
    "It turns out **state_dicts** can also be used for **checkpointing** a model, as we will see later down the line.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "fPlsgkPUemlh",
    "outputId": "b0f29d45-bb84-4fd4-9c69-9a7e17669450"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}},\n",
       " 'param_groups': [{'lr': 0.1,\n",
       "   'momentum': 0,\n",
       "   'dampening': 0,\n",
       "   'weight_decay': 0,\n",
       "   'nesterov': False,\n",
       "   'params': [0, 1]}]}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJk570c41MlG"
   },
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OlTBJ4H21WSj"
   },
   "source": [
    "**IMPORTANT**: we need to **send our model to the same device where the data is**. If our data is made of GPU tensors, our model must “live” inside the GPU as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nYJYMvYE1N-Q",
    "outputId": "a318eea6-eaa7-4625-c094-a3324d4fdfe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('b', tensor([0.3367], device='cuda:0')), ('w', tensor([0.1288], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = ManualLinearRegression().to(device)\n",
    "\n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sxg4yCviqhJT"
   },
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6NAAng-mqmdP"
   },
   "source": [
    "The **forward pass** is the moment when the model **makes predictions**.\n",
    "\n",
    "---\n",
    "\n",
    "You should **NOT call the `forward(x)`** method, though. You should **call the whole model itself**, as in **`model(x)`** to perform a forward pass and output predictions.\n",
    "\n",
    "---\n",
    "\n",
    "Otherwise, your model's _hooks_ will not work (if you have them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tuXlvnf-qlw_"
   },
   "outputs": [],
   "source": [
    "yhat = model(x_train_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QG2VxJhf1v_a"
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tbHFyO-22Amu"
   },
   "source": [
    "<h2><b><i>\"What does train() do? It only sets the mode!\"</b></i></h2>\n",
    "\n",
    "In PyTorch, models have a [**train()**](https://bit.ly/30VW2Ox) method which, somewhat disappointingly, **does NOT perform a training step**. Its only purpose is to **set the model to training mode**. \n",
    "\n",
    "Why is this important? Some models may use mechanisms like [**Dropout**](https://bit.ly/2X7v5pU), for instance, which have **distinct behaviors in training and evaluation phases**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Sl-Sd_1f1k--",
    "outputId": "311151e2-4017-4043-ae98-e8bb0c79f2a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('b', tensor([1.0235], device='cuda:0')), ('w', tensor([1.9690], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "# Now the optimizers uses the parameters from the model\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Sets model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Step 1\n",
    "    # No more manual prediction!\n",
    "    # yhat = b + w * x_tensor\n",
    "    yhat = model(x_train_tensor)\n",
    "    \n",
    "    # Step 2\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "    # Step 3\n",
    "    loss.backward()\n",
    "    # Step 4\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cD_yXamT2V7r"
   },
   "source": [
    "Now, the printed statements will look like this — final values for parameters **b** and **w** are still the same, so everything is ok :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGrzI0tw2pUX"
   },
   "source": [
    "### Nested Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hpp-gTuB23xK"
   },
   "source": [
    "In our model, we manually created two parameters to perform a linear regression. \n",
    "\n",
    "---\n",
    "\n",
    "You are **not** limited to defining parameters, though… **models can contain other models as its attributes** as well, so you can easily nest them. We’ll see an example of this shortly as well.\n",
    "\n",
    "---\n",
    "\n",
    "Let’s use PyTorch’s [**Linear**](https://bit.ly/2Ezu181) model as an attribute of our own, thus creating a nested model.\n",
    "\n",
    "Even though this clearly is a contrived example, as we are pretty much wrapping the underlying model without adding anything useful (or, at all!) to it, it illustrates well the concept.\n",
    "\n",
    "In the **`__init__`** method, we created an attribute that contains our **nested `Linear` model**.\n",
    "\n",
    "In the **`forward()`** method, we **call the nested model itself** to perform the forward pass (notice, we are **not** calling `self.linear.forward(x)`!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "amK4WCg72rVB"
   },
   "outputs": [],
   "source": [
    "class LayerLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Instead of our custom parameters, we use a Linear layer with single input and single output\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Now it only takes a call to the layer to make predictions\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cTaDWLJq3djj"
   },
   "source": [
    "Now, if we call the **parameters()** method of this model, **PyTorch will figure the parameters of its attributes in a recursive way**.\n",
    "\n",
    "You can also add new `Linear` attributes and, even if you don’t use them at all in the forward pass, they will **still** be listed under `parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "396v3N5A3ONO",
    "outputId": "b72889c5-4eef-422e-f56a-7ef2b44d477e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.2191]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.2018], requires_grad=True)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = LayerLinearRegression()\n",
    "\n",
    "list(dummy.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "lRDodDDg3XEs",
    "outputId": "cca27cc1-6e98-4037-9208-affec81e220a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight', tensor([[-0.2191]])),\n",
       "             ('linear.bias', tensor([0.2018]))])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KcS2nS2LxpI"
   },
   "source": [
    "### Layers\n",
    "\n",
    "A **Linear** model can be seen as a **layer** in a neural network.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/dvgodoy/PyTorch101_ODSC_Europe2022/main/images/layer.png\" width=\"50%\" height=\"50%\">\n",
    "</p>\n",
    "\n",
    "In the example above, the **hidden layer** would be `nn.Linear(3, 5)` and the **output layer** would be `nn.Linear(5, 1)`.\n",
    "\n",
    "\n",
    "There are **MANY** different layers that can be uses in PyTorch:\n",
    "- [Convolution Layers](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n",
    "- [Pooling Layers](https://pytorch.org/docs/stable/nn.html#pooling-layers)\n",
    "- [Padding Layers](https://pytorch.org/docs/stable/nn.html#padding-layers)\n",
    "- [Non-linear Activations](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
    "- [Normalization Layers](https://pytorch.org/docs/stable/nn.html#normalization-layers)\n",
    "- [Recurrent Layers](https://pytorch.org/docs/stable/nn.html#recurrent-layers)\n",
    "- [Transformer Layers](https://pytorch.org/docs/stable/nn.html#transformer-layers)\n",
    "- [Linear Layers](https://pytorch.org/docs/stable/nn.html#linear-layers)\n",
    "- [Dropout Layers](https://pytorch.org/docs/stable/nn.html#dropout-layers)\n",
    "- [Sparse Layers (embbedings)](https://pytorch.org/docs/stable/nn.html#sparse-layers)\n",
    "- [Vision Layers](https://pytorch.org/docs/stable/nn.html#vision-layers)\n",
    "- [DataParallel Layers (multi-GPU)](https://pytorch.org/docs/stable/nn.html#dataparallel-layers-multi-gpu-distributed)\n",
    "- [Flatten Layer](https://pytorch.org/docs/stable/nn.html#flatten)\n",
    "\n",
    "We have just used a **Linear** layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X7G3-9GX2sRe"
   },
   "source": [
    "### Sequential Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8G3evlAo37Gj"
   },
   "source": [
    "<h2><b><i>Run-of-the-mill layers? Sequential model!</b></i></h2>\n",
    "\n",
    "Our model was simple enough… You may be thinking: “*why even bother to build a class for it?!*” Well, you have a point…\n",
    "\n",
    "For **straightforward models**, that use **run-of-the-mill layers**, where the output of a layer is sequentially fed as an input to the next, we can use a, er… [**Sequential**](https://bit.ly/3hRQTxP) model :-)\n",
    "\n",
    "In our case, we would build a Sequential model with a single argument, that is, the Linear layer we used to train our linear regression. The model would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mt9ssm5w2vQa"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WZt_4Z3U4KpE"
   },
   "source": [
    "Simple enough, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgJ1Oi1R4RAF"
   },
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uZ_wUvVA4TEP"
   },
   "source": [
    "So far, we’ve defined:\n",
    "* an **optimizer**\n",
    "\n",
    "* a **loss function**\n",
    "\n",
    "* a **model**\n",
    "\n",
    "Scroll up a bit and take a quick look at the code inside the loop. Would it **change** if we were using a **different optimizer**, or **loss**, or even **model**? If not, how can we make it more generic?\n",
    "\n",
    "Well, I guess we could say all these lines of code **perform a training step**, given those **three elements** (optimizer, loss and model),the **features** and the **labels**.\n",
    "\n",
    "So, how about **writing a function that takes those three elements** and **returns another function that performs a training step**, taking a set of features and labels as arguments and returning the corresponding loss?\n",
    "\n",
    "*For an overview of higher-order functions, check my recently published post: [Functions That Return Functions: Higher-Order Functions and Decorators in Python with Examples](https://towardsdatascience.com/functions-that-return-functions-higher-order-functions-and-decorators-in-python-with-examples-4282742cdd3e)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step_fn(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def perform_train_step_fn(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        \n",
    "        # Step 1 - Computes our model's predicted output - forward pass\n",
    "        yhat = model(x)\n",
    "        # Step 2 - Computes the loss\n",
    "        loss = loss_fn(yhat, y)\n",
    "        # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "        loss.backward()\n",
    "        # Step 4 - Updates parameters using gradients and the learning rate\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    # Returns the function that will be called inside the train loop\n",
    "    return perform_train_step_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q1z-JIko4ysl"
   },
   "source": [
    "Then we can use this general-purpose function to build a **train_step_fn()** function to be called inside our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.make_train_step_fn.<locals>.perform_train_step_fn(x, y)>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(13)\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "train_step_fn = make_train_step_fn(model, loss_fn, optimizer)\n",
    "train_step_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[-0.8164]], device='cuda:0')),\n",
       "             ('0.bias', tensor([-0.0412], device='cuda:0'))])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.403440952301025"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step_fn(x_train_tensor, y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[-0.5392]], device='cuda:0')),\n",
       "             ('0.bias', tensor([0.4349], device='cuda:0'))])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4F60DNI64zBk",
    "outputId": "8d5eb83c-e276-4fc1-d840-b68f7cd5dfe9"
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "## Model Configuration ##\n",
    "#########################\n",
    "lr = 1e-1\n",
    "torch.manual_seed(13)\n",
    "\n",
    "# Create a MODEL, a LOSS FUNCTION and an OPTIMIZER\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EkqKUEaj5EOi"
   },
   "source": [
    "Now our code should look like this… see how **tiny** the training loop is now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "p2ysa76u44nX",
    "outputId": "e5d8df37-4578-4373-fd7b-771d531182b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[1.9690]], device='cuda:0')), ('0.bias', tensor([1.0235], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "## Model Training ##\n",
    "####################\n",
    "\n",
    "# Creates the train_step function for our model, loss function and optimizer\n",
    "train_step_fn = make_train_step_fn(model, loss_fn, optimizer)\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "losses = []\n",
    "# For each epoch...\n",
    "for epoch in range(n_epochs):\n",
    "    # Performs one train step and returns the corresponding loss\n",
    "    loss = train_step_fn(x_train_tensor, y_train_tensor)\n",
    "    losses.append(loss)\n",
    "    \n",
    "# Checks model's parameters\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "colab_type": "code",
    "id": "jOnHnxJQDU5C",
    "outputId": "20b12834-b138-4fe5-fdcf-7c609a29745f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses[:200])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'display_quiz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdisplay_quiz\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#/quiz/quiz4.b64\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'display_quiz' is not defined"
     ]
    }
   ],
   "source": [
    "display_quiz('#/quiz/quiz4.b64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
