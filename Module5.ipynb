{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using Google Colab, please run these commands first\n",
    "# !wget https://github.com/dvgodoy/PyTorch101_AI_Plus/raw/main/quiz.zip -O quiz.zip\n",
    "# !unzip -qo quiz.zip\n",
    "# !wget https://raw.githubusercontent.com/dvgodoy/PyTorch101_AI_Plus/main/simple_linear_regression.py -O simple_linear_regression.py\n",
    "# !wget https://raw.githubusercontent.com/dvgodoy/PyTorch101_AI_Plus/main/v4.py -O v4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"<style>.container { width:90% !important; }.text_cell_render, .output_text {font-family: Lato;font-size: 18px;line-height: 1.5;}.CodeMirror {font-size: 16px;}</style>\"\"\"))\n",
    "from quiz.jupyterquiz import display_quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i simple_linear_regression.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step_fn(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def perform_train_step_fn(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        \n",
    "        # Step 1 - Computes our model's predicted output - forward pass\n",
    "        yhat = model(x)\n",
    "        # Step 2 - Computes the loss\n",
    "        loss = loss_fn(yhat, y)\n",
    "        # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "        loss.backward()\n",
    "        # Step 4 - Updates parameters using gradients and the learning rate\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    # Returns the function that will be called inside the train loop\n",
    "    return perform_train_step_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VcHv7EZ56PR"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GUdT1l_Q6Dx_"
   },
   "source": [
    "In PyTorch, a **dataset** is represented by a regular **Python class** that inherits from the [**Dataset**](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class. You can think of it as a kind of a Python **list of tuples**, each tuple corresponding to **one point (features, label)**.\n",
    "\n",
    "The most fundamental methods it needs to implement are:\n",
    "\n",
    "* **`__init__(self)`**: it takes **whatever arguments** needed to build a **list of tuples** — it may be the name of a CSV file that will be loaded and processed; it may be two tensors, one for features, another one for labels; or anything else, depending on the task at hand.\n",
    "\n",
    "* **`__getitem__(self, index)`**: it allows the dataset to be **indexed**, so it can work like a list (`dataset[i]`) — it must **return a tuple (features, label)** corresponding to the requested data point. We can either return the **corresponding slices** of our **pre-loaded** dataset or tensors or, as mentioned above, **load them on demand** (like in this [example](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class)).\n",
    "\n",
    "* **`__len__(self)`**: it should simply return the **size** of the whole dataset so, whenever it is sampled, its indexing is limited to the actual size.\n",
    "\n",
    "---\n",
    "\n",
    "There is **no need to load the whole dataset in the constructor method** (`__init__`). If your **dataset is big** (tens of thousands of image files, for instance), loading it at once would not be memory efficient. It is recommended to **load them on demand** (whenever `__get_item__` is called).\n",
    "\n",
    "---\n",
    "\n",
    "Let’s build a simple custom dataset that takes two tensors as arguments: one for the features, one for the labels. For any given index, our dataset class will return the corresponding slice of each of those tensors. It should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eKu-E5xQ57pE"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "T2Lpj43e7Gpt",
    "outputId": "e73b01f6-84bc-4603-e0b4-c0c0ba51a0f7"
   },
   "outputs": [],
   "source": [
    "# Wait, is this a CPU tensor now? Why? Where is .to(device)?\n",
    "x_train_tensor = torch.as_tensor(x_train).float()\n",
    "y_train_tensor = torch.as_tensor(y_train).float()\n",
    "\n",
    "train_data = CustomDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zW1bTD4l7VAE"
   },
   "source": [
    "---\n",
    "\n",
    "Did you notice we built our **training tensors** out of Numpy arrays but we **did not send them to a device**? So, they are **CPU** tensors now! **Why**?\n",
    "\n",
    "We **don’t want our whole training data to be loaded into GPU tensors**, as we have been doing in our example so far, because **it takes up space** in our precious **graphics card’s RAM**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EUwVUhKP75zt"
   },
   "source": [
    "### TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a0-lPsQf770A"
   },
   "source": [
    "Besides, you may be thinking “*why go through all this trouble to wrap a couple of tensors in a class?*”. And, once again, you do have a point… if a dataset is nothing else but a **couple of tensors**, we can use PyTorch’s [**TensorDataset**](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) class, which will do pretty much what we did in our custom dataset above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wIjLRR_666H2",
    "outputId": "08e88416-cf08-41b3-d297-b65acc07f5bc"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9bZZI1U37ylE"
   },
   "source": [
    "OK, fine, but then again, **why** are we building a dataset anyway? We’re doing it because we want to use a…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLvd96o77-Rj"
   },
   "source": [
    "## DataLoader, splitting your data into mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JH_FSh4G8CM-"
   },
   "source": [
    "<h2><b><i>- Let's split data into mini-batches<br>- Use DataLoaders!</b></i></h2>\n",
    "\n",
    "Until now, we have used the **whole training data** at every training step. It has been **batch gradient descent** all along. This is fine for our *ridiculously small dataset*, sure, but if we want to go serious about all this, we **must use mini-batch** gradient descent. Thus, we need mini-batches. Thus, we need to **slice** our dataset accordingly. \n",
    "\n",
    "Do you want to do it *manually*?! Me neither!\n",
    "\n",
    "So we use PyTorch’s [**DataLoader**](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class for this job. We tell it which **dataset** to use (the one we just built in the previous section), the desired **mini-batch size** and if we’d like to **shuffle** it or not. That’s it!\n",
    "\n",
    "---\n",
    "\n",
    "There is more to a `DataLoader` than meets the eye—it is also possible to use it together with a **sampler** to fetch mini-batches that compensate for **imbalanced classes**, for instance. Too much to handle right now, but we will eventually get there.\n",
    "\n",
    "---\n",
    "\n",
    "Our **loader** will behave like an **iterator**, so we can **loop over it** and **fetch a different mini-batch** every time.\n",
    "\n",
    "*Since PyTorch 1.7, in order to ensure reproducibility, we need to **assign a generator to the DataLoader**, so it is used in the corresponding sampler (provided it uses a generator, of course).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ana8r6AN7_hv"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True, generator=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RHHtcorn8dMq"
   },
   "source": [
    "To retrieve a mini-batch, one can simply run the command below — it will return a list containing two tensors, one for the features, another one for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "id": "sPX5ggEC8bXU",
    "outputId": "50146492-76fe-49a7-cf3d-bc7bd3ca43a5"
   },
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wrhy1id98iVj"
   },
   "source": [
    "How does this change our training loop? Let’s check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "madfwBfC8fjs",
    "outputId": "027fa3a5-ba20-4f4f-c124-bbf741edc321"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#########################\n",
    "## Model Configuration ##\n",
    "#########################\n",
    "\n",
    "lr = 1e-1\n",
    "\n",
    "# Create a MODEL, a LOSS FUNCTION and an OPTIMIZER\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "####################\n",
    "## Model Training ##\n",
    "####################\n",
    "\n",
    "# Creates the train_step function for our model, loss function and optimizer\n",
    "train_step_fn = make_train_step_fn(model, loss_fn, optimizer)\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # inner loop\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # the dataset \"lives\" in the CPU, so do our mini-batches\n",
    "        # therefore, we need to send those mini-batches to the\n",
    "        # device where the model \"lives\"\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        loss = train_step_fn(x_batch, y_batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "colab_type": "code",
    "id": "0_QL4Q2aEGR7",
    "outputId": "c1d7d27d-793f-439e-9437-2511f2818d76"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs (?)')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice it is taking **longer** to train now? Can you guess **why**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things are different now: not only we have an **inner loop** to load each and every **mini-batch** from our **DataLoader** but, more importantly, we are now **sending only one mini-batch to the device**.\n",
    "\n",
    "---\n",
    "\n",
    "For bigger datasets, **loading data sample by sample** (into a **CPU** tensor) using **Dataset’s \\__getitem__** and then **sending all samples** that belong to the **same mini-batch at once to your GPU** (device) is the way to go in order to make the **best use of your graphics card’s RAM**.\n",
    "\n",
    "Moreover, if you have **many GPUs** to train your model on, it is best to keep your dataset “agnostic” and assign the batches to different GPUs during training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6mc06yWq85XN"
   },
   "source": [
    "\n",
    "So far, we’ve focused on the **training data** only. We built a *dataset* and a *data loader* for it. We can do the same for the **validation** data, using the **split** we performed at the beginning of this training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_g3S4hx83pW"
   },
   "outputs": [],
   "source": [
    "######################\n",
    "## Data Preparation ##\n",
    "######################\n",
    "x_train_tensor = torch.as_tensor(x_train).float()\n",
    "y_train_tensor = torch.as_tensor(y_train).float()\n",
    "\n",
    "x_val_tensor = torch.as_tensor(x_val).float()\n",
    "y_val_tensor = torch.as_tensor(y_val).float()\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "# builds a loader of each set\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8pD8ItwP-TMC"
   },
   "source": [
    "Now we have a **data loader** for our **validation** set, so, it makes sense to use it for the…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d6iCpQ1B-YfS"
   },
   "source": [
    "## Evaluation: does it generalize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aQCWnko4-az9"
   },
   "source": [
    "Now, we need to change the training loop to include the **evaluation of our model**, that is, computing the **validation loss**. The first step is to include another inner loop to handle the *mini-batches* that come from the *validation loader* , sending them to the same *device* as our model. Next, we make **predictions** using our model and compute the corresponding **loss**.\n",
    "\n",
    "That’s pretty much it, but there are **two small, yet important**, things to consider:\n",
    "\n",
    "* [**torch.no_grad()**](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad): even though it won’t make a difference in our simple model, it is a **good practice to wrap the validation inner loop with this context manager to disable any gradient calculation** that you may inadvertently trigger — **gradients belong in training**, not in validation steps;\n",
    "    \n",
    "* [**eval()**](https://bit.ly/3ge8X4H): the only thing it does is **setting the model to evaluation mode** (just like its `train()` counterpart did), so the model can adjust its behavior regarding some operations, like [**Dropout**](https://bit.ly/3346fuU).\n",
    "\n",
    "So, we build a higher-order function that returns a **validation step function**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_val_step_fn(model, loss_fn):\n",
    "    # Builds function that performs a step in the validation loop\n",
    "    def perform_val_step_fn(x, y):\n",
    "        # Sets model to EVAL mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Step 1 - Computes our model's predicted output - forward pass\n",
    "        yhat = model(x)\n",
    "        # Step 2 - Computes the loss\n",
    "        loss = loss_fn(yhat, y)\n",
    "        # There is no need to compute Steps 3 and 4, since we don't update parameters during evaluation\n",
    "        return loss.item()\n",
    "    \n",
    "    return perform_val_step_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4bgvUJLI-Z_D",
    "outputId": "492f2366-293c-423a-b130-e59f0fda8a1c"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "######################\n",
    "## Data Preparation ##\n",
    "######################\n",
    "x_train_tensor = torch.as_tensor(x_train).float()\n",
    "y_train_tensor = torch.as_tensor(y_train).float()\n",
    "\n",
    "x_val_tensor = torch.as_tensor(x_val).float()\n",
    "y_val_tensor = torch.as_tensor(y_val).float()\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "# builds a loader of each set\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True, generator=g)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)\n",
    "\n",
    "#########################\n",
    "## Model Configuration ##\n",
    "#########################\n",
    "\n",
    "# defines learning rate\n",
    "lr = 1e-1\n",
    "\n",
    "# Create a MODEL, a LOSS FUNCTION and an OPTIMIZER\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "####################\n",
    "## Model Training ##\n",
    "####################\n",
    "\n",
    "# Creates the train_step function for our model, loss function and optimizer\n",
    "train_step_fn = make_train_step_fn(model, loss_fn, optimizer)\n",
    "\n",
    "# Creates the val_step function for our model and loss function\n",
    "val_step_fn = make_val_step_fn(model, loss_fn)\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # inner loop\n",
    "    mini_batch_losses = []\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        mini_batch_loss = train_step_fn(x_batch, y_batch)\n",
    "        mini_batch_losses.append(mini_batch_loss)\n",
    "\n",
    "    loss = np.mean(mini_batch_losses)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # VALIDATION\n",
    "    # no gradients in validation!\n",
    "    with torch.no_grad():\n",
    "        mini_batch_losses = []\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            mini_batch_loss = val_step_fn(x_batch, y_batch)\n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    "\n",
    "        val_loss = np.mean(mini_batch_losses)\n",
    "        val_losses.append(val_loss) \n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our code should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "HQsR61egsV5P",
    "outputId": "fcbd0e9a-de35-4dd2-ba27-24043b085c49"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rmmux1GVseDH"
   },
   "source": [
    "\"*Wait, there is something weird with this plot...*\", you say. You're right, the **validation loss** is **smaller** than the **training loss**. Shouldn't it be the other way around?! Well, generally speaking, *YES*, it should... but you can learn more about situations where this *swap* happens at this great [post](https://www.pyimagesearch.com/2019/10/14/why-is-my-validation-loss-lower-than-my-training-loss/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ysxlX3K69G9G"
   },
   "source": [
    "## Saving (and Loading) Models: taking a break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6j9bvL8K94JF"
   },
   "source": [
    "<h2><b><i>\"That would be great, to restart training later\"</b></i></h2>\n",
    "\n",
    "So, it is important to be able to **checkpoint** our model, in case we'd like to **restart training later**.\n",
    "\n",
    "To checkpoint a model, we basically have to **save its state** into a file, to **load** it back later - nothing special, actually.\n",
    "\n",
    "What defines the **state of a model**?\n",
    "- **model.state_dict()**: kinda obvious, right?\n",
    "- **optimizer.state_dict()**: remember optimizers had the `state_dict` as well?\n",
    "- **loss**: after all, you should keep track of its evolution\n",
    "- **epoch**: it is just a number, so why not? :-)\n",
    "- **anything else you'd like to have restored**\n",
    "\n",
    "Then, **wrap everything into a Python dictionary** and use [**torch.save()**](https://bit.ly/3jQOnJY) to dump it all into a file! Easy peasy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8gbfTwXn97m9"
   },
   "outputs": [],
   "source": [
    "checkpoint = {'epoch': n_epochs,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': losses,\n",
    "              'val_loss': val_losses}\n",
    "\n",
    "torch.save(checkpoint, 'model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pan0KRCt9_4s"
   },
   "source": [
    "How would you **load** it back? Easy as well:\n",
    "- load the dictionary back using [**torch.load()**](https://bit.ly/33jk9tp)\n",
    "- load **model** and **optimizer** state dictionaries back using its methods [**load_state_dict()**](https://bit.ly/338dRwq)\n",
    "- load everything else into their corresponding variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LWdi-mOf9_K7"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('model_checkpoint.pth')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "epoch = checkpoint['epoch']\n",
    "losses = checkpoint['loss']\n",
    "val_losses = checkpoint['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AvwaHaAP-Mgk"
   },
   "source": [
    "You may save a model for **checkpointing**, like we have just done, or for **making predictions**, assuming training is finished.\n",
    "\n",
    "After loading the model, **DO NOT FORGET**:\n",
    "\n",
    "---\n",
    "\n",
    "**SET THE MODE**:\n",
    "- **checkpointing: model.train()**\n",
    "- **predicting: model.eval()**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz('#./quiz/quiz5.b64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from v4 import StepByStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "######################\n",
    "## Data Preparation ##\n",
    "######################\n",
    "x_train_tensor = torch.as_tensor(x_train).float()\n",
    "y_train_tensor = torch.as_tensor(y_train).float()\n",
    "\n",
    "x_val_tensor = torch.as_tensor(x_val).float()\n",
    "y_val_tensor = torch.as_tensor(y_val).float()\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "# builds a loader of each set\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True, generator=g)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)\n",
    "\n",
    "#########################\n",
    "## Model Configuration ##\n",
    "#########################\n",
    "\n",
    "# defines learning rate\n",
    "lr = 1e-1\n",
    "\n",
    "# Create a MODEL, a LOSS FUNCTION and an OPTIMIZER\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "####################\n",
    "## Model Training ##\n",
    "####################\n",
    "sbs = StepByStep(model, loss_fn, optimizer)\n",
    "sbs.set_loaders(train_loader, val_loader)\n",
    "sbs.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sbs.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7v_kLDpO_1kn"
   },
   "source": [
    "## Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_6_ZPJMa_5DI"
   },
   "source": [
    "I believe this training has **most of the necessary steps** one needs go to trough in order to **learn**, in a **structured** and **incremental** way, how to **develop Deep Learning models using PyTorch**.\n",
    "\n",
    "Hopefully, after finishing working through all code in this post, you’ll be able to better appreciate and more easily work your way through PyTorch’s official [tutorials](https://pytorch.org/tutorials/).\n",
    "\n",
    "If you'd like to connect, you can find me on [LinkedIn](https://br.linkedin.com/in/dvgodoy) or [Twitter](https://twitter.com/dvgodoy).\n",
    "\n",
    "<h1><center>THANK YOU!</center></h1>\n",
    "\n",
    "<h2><center>\n",
    "    For more information on my series of books, please check its official website:\n",
    "<br>\n",
    "<br>\n",
    "<a href=\"https://pytorchstepbystep.com\">\n",
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/dvgodoy/PyTorch101_AI_Plus/main/images/book_cover.png\" width=\"60%\">\n",
    "</p>\n",
    "<br>\n",
    "    https://pytorchstepbystep.com\n",
    "</a>\n",
    "</center></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
