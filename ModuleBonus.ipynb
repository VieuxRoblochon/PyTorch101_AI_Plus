{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using Google Colab, please run these commands first\n",
    "# !wget https://github.com/dvgodoy/PyTorch101_AI_Plus/raw/main/quiz.zip -O quiz.zip\n",
    "# !unzip -qo quiz.zip\n",
    "# !mkdir plots\n",
    "# !wget https://raw.githubusercontent.com/dvgodoy/PyTorch101_AI_Plus/main/plots/chapter3.py -O ./plots/chapter3.py\n",
    "# !mkdir v4\n",
    "# !wget https://raw.githubusercontent.com/dvgodoy/PyTorch101_AI_Plus/main/v4/StepByStep.py -O ./v4/StepByStep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from plots.chapter3 import *\n",
    "from v4 import StepByStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are my data points separable?\n",
    "\n",
    "That's the million-dollar question!\n",
    "\n",
    "What happens if the points are **not separable at all**? Let's take a quick detour and look at another tiny dataset with 10 data points, seven red, three blue. The colors are the **labels (_y_)**, and each data point has a **single feature (*x<sub>1</sub>*)**. We could plot them **along a line**; after all, we have only **one dimension**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([-2.8, -2.2, -1.8, -1.3, -.4, 0.3, 0.6, 1.3, 1.9, 2.5])\n",
    "y = np.array([0., 0., 0., 0., 1., 1., 1., 0., 0., 0.])\n",
    "\n",
    "fig = one_dimension(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you **separate the blue points from the red ones with one straight line**? Obviously notâ€”these points **are not separable** (in one dimension, that is).\n",
    "\n",
    "Should we give up, then?\n",
    "\n",
    "---\n",
    "\n",
    "\"***Never give up, never surrender!***\"\n",
    "\n",
    "[Commander Taggart](https://i.imgflip.com/51toa2.jpg)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "If it doesn't work in one dimension, try using two! There is just one problem, though: Where does the other dimension come from? We can use a **trick** here: We apply a **function** to the **original dimension (feature)** and use the result as a **second dimension (feature)**. Quite simple, right?\n",
    "\n",
    "For the tiny dataset at hand, we could try the **square function**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large X_2 = f(X_1)= X_1^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = two_dimensions(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the original question: \"_Can you separate the blue points from the red ones with one straight line?_\"\n",
    "\n",
    "In two dimensions, that's a piece of cake!\n",
    "\n",
    "---\n",
    "\n",
    "**The more dimensions, the more separable the points are.**\n",
    "\n",
    "---\n",
    "\n",
    "It is beyond the scope of this module to explain _why_ this trick works. The important thing is to **understand the general idea**: As the **number of dimensions increases**, there is **more and more empty space**. If the data points are farther apart, it is likely easier to separate them. In two dimensions, the decision boundary is a line. In three dimensions, it is a plane. In four dimensions and more, it is a hyper-plane (fancier wording for a plane you can't draw).\n",
    "\n",
    "Have you heard of the **kernel trick** for support vector machines (SVMs)? That's pretty much what it does! The **kernel** is nothing but the **function** we use to create additional dimensions. The square function we used is a **polynomial**, so we used a **polynomial kernel**.\n",
    "\n",
    "\"*Why are we talking about SVMs?*\"\n",
    "\n",
    "Excellent question! It turns out **neural networks** may also **increase the dimensionality**. That's what happens if you add a **hidden layer** with **more units** than the **number of features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "\n",
    "dummy_x = torch.as_tensor(x).view(-1, 1).float()\n",
    "dummy_y = torch.as_tensor(y).view(-1, 1).float()\n",
    "\n",
    "dummy_ds = TensorDataset(dummy_x, dummy_y)\n",
    "dummy_dl = DataLoader(dummy_ds, batch_size=5, shuffle=True)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#########################\n",
    "## Model Configuration ##\n",
    "#########################\n",
    "lr = 1e-1\n",
    "dummy_model = nn.Sequential()\n",
    "dummy_model.add_module('hidden', nn.Linear(1, 2))\n",
    "dummy_model.add_module('activation', nn.ReLU())\n",
    "dummy_model.add_module('output', nn.Linear(2, 1))\n",
    "dummy_model.add_module('sigmoid', nn.Sigmoid())\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.SGD(dummy_model.parameters(), lr)\n",
    "\n",
    "####################\n",
    "## Model Training ##\n",
    "####################\n",
    "sbs = StepByStep(dummy_model, loss_fn, optimizer)\n",
    "sbs.set_loaders(dummy_dl)\n",
    "sbs.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model above increases dimensionality **from one dimension** (one feature) to **two dimensions** and then uses those **ten dimensions to compute logits**. But it **only works if there is an activation function between the layers**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model(dummy_x), dummy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = dummy_model.output.state_dict()\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large x_2 = -\\frac{b}{w_2} - \\frac{w_1}{w_2}x_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (-out['bias']/out['weight'][0][1]).numpy()\n",
    "w = (-out['weight'][0][0]/out['weight'][0][1]).numpy()\n",
    "boundary = lambda x: b + w * x\n",
    "b, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_feat = nn.ReLU()(dummy_model.hidden(dummy_x)).tolist()\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.scatter(*np.array(hidden_feat)[y==0].T, c='red')\n",
    "ax.scatter(*np.array(hidden_feat)[y==1].T, c='blue')\n",
    "ax.plot([0, 6], [boundary(0), boundary(6)])\n",
    "ax.set_xlim([-.2, 6])\n",
    "ax.set_ylim([-.2, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure7(nn.ReLU()(dummy_model.hidden(dummy_x)).detach(), \n",
    "              dummy_y, \n",
    "              nn.Sequential(*list(dummy_model.children())[1:-1]), \n",
    "              'cpu', x_range=(-.1, 6), y_range=(-.1, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
