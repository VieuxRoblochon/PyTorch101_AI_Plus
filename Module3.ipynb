{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using Google Colab, please run these commands first\n",
    "# !wget https://github.com/dvgodoy/PyTorch101_AI_Plus/raw/main/quiz.zip -O quiz.zip\n",
    "# !unzip -qo quiz.zip\n",
    "# !wget https://raw.githubusercontent.com/dvgodoy/PyTorch101_AI_Plus/main/simple_linear_regression.py -O simple_linear_regression.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"<style>.container { width:90% !important; }.text_cell_render, .output_text {font-family: Lato;font-size: 18px;line-height: 1.5;}.CodeMirror {font-size: 16px;}</style>\"\"\"))\n",
    "from quiz.jupyterquiz import display_quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --quiet torchviz\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i simple_linear_regression.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "x_train_tensor = torch.as_tensor(x_train).float().to(device)\n",
    "y_train_tensor = torch.as_tensor(y_train).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "68GypByGi8RO"
   },
   "source": [
    "## Autograd, your companion for all your gradient needs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sPV5It5YjAvL"
   },
   "source": [
    "Autograd is PyTorch’s *automatic differentiation package*. Thanks to it, we **don’t need to worry** about partial derivatives, chain rule or anything like it.\n",
    "\n",
    "<h2><b><i>Computing gradients manually?! No way! Backward!</b></i></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLIcMHgVo4CU"
   },
   "source": [
    "### backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irQuMOgJo6FU"
   },
   "source": [
    "So, how do we tell PyTorch to do its thing and **compute all gradients**? That’s what [**backward()**](https://bit.ly/3eV9Dub) is good for.\n",
    "\n",
    "Do you remember the **starting point** for **computing the gradients**? It was the **loss**, as we computed its partial derivatives w.r.t. our parameters. Hence, we need to invoke the `backward()` method from the corresponding Python variable, like, `loss.backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q7Bi9KWgmuXI"
   },
   "outputs": [],
   "source": [
    "# Step 0\n",
    "torch.manual_seed(42)\n",
    "\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9OilmlLh9_d"
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Computes our model's predicted output\n",
    "yhat = b + w * x_train_tensor\n",
    "\n",
    "# Step 2    \n",
    "# How wrong is our model? That's the error! \n",
    "error = (yhat - y_train_tensor)\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# Step 3    \n",
    "# No more manual computation of gradients! \n",
    "loss.backward()\n",
    "\n",
    "# Computes gradients for both \"b\" and \"w\" parameters\n",
    "# b_grad = 2 * error.mean()\n",
    "# w_grad = 2 * (x_train_tensor * error).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "youD0_1Do9_E"
   },
   "source": [
    "### grad / zero_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z2JJ_osngW5H"
   },
   "source": [
    "\n",
    "What about the **actual values** of the **gradients**? We can inspect them by looking at the [**grad**](https://bit.ly/3fYtNFa) **attribute** of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SM1hYBxlhMoZ",
    "outputId": "06042d01-680b-4746-a270-b31e66e0bc59"
   },
   "outputs": [],
   "source": [
    "print(b.grad, w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qJL6O06ChLio"
   },
   "source": [
    "If you check the method’s documentation, it clearly states that **gradients are accumulated**. \n",
    "\n",
    "You can check this out by running the two code cells above again.\n",
    "\n",
    "So, every time we use the **gradients** to **update** the parameters, we need to **zero the gradients afterwards**. And that’s what [**zero_()**](https://pytorch.org/docs/stable/generated/torch.Tensor.zero_.html#torch.Tensor.zero_) is good for.\n",
    "\n",
    "---\n",
    "\n",
    "*In PyTorch, every method that **ends** with an **underscore (_)** makes changes **in-place**, meaning, they will **modify** the underlying variable.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3wh62U94i7-o",
    "outputId": "23c746f0-fce0-49ec-f45f-8555b2ac97f5"
   },
   "outputs": [],
   "source": [
    "b.grad.zero_(), w.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5dy5vKAOi8J1"
   },
   "source": [
    "So, let’s **ditch** the **manual computation of gradients** and use both `backward()` and `zero_()` methods instead.\n",
    "\n",
    "And, we are still missing **Step 4**, that is, **updating the parameters**. Let's include it as well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "2GAkYJYniX42",
    "outputId": "b0dd52a6-125a-4287-82ec-f16e382fe59a"
   },
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "\n",
    "# Step 0\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Step 1\n",
    "# Computes our model's predicted output\n",
    "yhat = b + w * x_train_tensor\n",
    "\n",
    "# Step 2    \n",
    "# How wrong is our model? That's the error! \n",
    "error = (yhat - y_train_tensor)\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# Step 3    \n",
    "# No more manual computation of gradients! \n",
    "loss.backward()\n",
    "# Computes gradients for both \"b\" and \"w\" parameters\n",
    "# b_grad = 2 * error.mean()\n",
    "# w_grad = 2 * (x_train_tensor * error).mean()\n",
    "print(b.grad, w.grad)\n",
    "\n",
    "# Step 4\n",
    "# Updates parameters using gradients and the learning rate\n",
    "with torch.no_grad(): # what is that?!\n",
    "    b -= lr * b.grad\n",
    "    w -= lr * w.grad\n",
    "\n",
    "# PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go...\n",
    "b.grad.zero_()\n",
    "w.grad.zero_()\n",
    "\n",
    "print(b.grad, w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-spqiSkpG3_"
   },
   "source": [
    "### no_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SIhiT37ApJy3"
   },
   "source": [
    "<h2><b><i>\"One does not simply update parameters without no_grad\"</b></i></h2>\n",
    "\n",
    "Why do we need to use [**no_grad()**](https://bit.ly/39o0nh4) to **update the parameters**?\n",
    "\n",
    "The culprit is PyTorch’s ability to build a **dynamic computation graph** from every **Python operation** that involves any **gradient-computing tensor** or its **dependencies**.\n",
    "\n",
    "---\n",
    "\n",
    "**What is a dynamic computation graph?**\n",
    "\n",
    "Don't worry, we’ll go deeper into the inner workings of the dynamic computation graph in the next section.\n",
    "\n",
    "---\n",
    "\n",
    "So, how do we tell PyTorch to “**back off**” and let us **update our parameters** without messing up with its **fancy dynamic computation graph**? \n",
    "\n",
    "\n",
    "That is the purpose of **no_grad()**: it allows us to **perform regular Python operations on tensors, independent of PyTorch’s computation graph**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rxkh-X1ui9qJ",
    "outputId": "c5d3cc14-a26e-4bcc-b6d1-edde36a28765"
   },
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "# Step 0\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1\n",
    "    # Computes our model's predicted output\n",
    "    yhat = b + w * x_train_tensor\n",
    "\n",
    "    # Step 2    \n",
    "    # How wrong is our model? That's the error! \n",
    "    error = (yhat - y_train_tensor)\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3    \n",
    "    # No more manual computation of gradients! \n",
    "    loss.backward()\n",
    "\n",
    "    # Step 4\n",
    "    # Updates parameters using gradients and the learning rate\n",
    "    with torch.no_grad():\n",
    "        b -= lr * b.grad\n",
    "        w -= lr * w.grad\n",
    "\n",
    "    # PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go...\n",
    "    b.grad.zero_()\n",
    "    w.grad.zero_()\n",
    "\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cfde7qBr-dLd"
   },
   "source": [
    "Finally, we managed to successfully run our model and get the **resulting parameters**. Surely enough, they **match** the ones we got in our *Numpy*-only implementation.\n",
    "\n",
    "Let's take a look at the **loss** at the end of the training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qyrLZ7bm-fu3",
    "outputId": "6e292fe9-409b-4bad-e05e-7308b1385acb"
   },
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "shzUgjPH_hqq"
   },
   "source": [
    "What if we wanted to have it as a *Numpy* array? I guess we could just use **numpy()** again, right? (and **cpu()** as well, since our *loss* is in the `cuda` device..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "c-fSotmZ_s-9",
    "outputId": "1b52b633-68ab-4077-a4f5-53122695136e"
   },
   "outputs": [],
   "source": [
    "loss.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Uwg76k-_6CH"
   },
   "source": [
    "What happened here? Unlike our *data tensors*, the **loss tensor** is actually computing gradients - and in order to use **numpy**, we need to [**detach()**](https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html#torch.Tensor.detach) that tensor from the computation graph first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qWBiGTaLAVfj",
    "outputId": "5372bb61-7f59-4f68-e4bd-f9fdeffd68ef"
   },
   "outputs": [],
   "source": [
    "loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mVvbZu_6AZZU"
   },
   "source": [
    "This seems like **a lot of work**, there must be an easier way! And there is one indeed: we can use [**item()**](https://pytorch.org/docs/stable/generated/torch.Tensor.item.html#torch.Tensor.item), for **tensors with a single element** or [**tolist()**](https://pytorch.org/docs/stable/generated/torch.Tensor.tolist.html#torch.Tensor.tolist) otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PCKOS0RlAx-U",
    "outputId": "fd2bee76-87d3-4b08-d575-d2cb009e4fe5"
   },
   "outputs": [],
   "source": [
    "print(loss.item(), loss.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mL4F2K3KpadZ"
   },
   "source": [
    "## Dynamic Computation Graph: what is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EamDiupxpjSp"
   },
   "source": [
    "<h2><b><i>\"No one can be told what the dynamic computation graph is - you have to see it for yourself\"</b></i></h2>\n",
    "\n",
    "Jokes aside, I want **you** to **see the graph for yourself** too!\n",
    "\n",
    "The [PyTorchViz](https://github.com/szagoruyko/pytorchviz) package and its `make_dot(variable)` method allows us to easily visualize a graph associated with a given Python variable.\n",
    "\n",
    "So, let’s stick with the **bare minimum**: two (gradient computing) **tensors** for our parameters, predictions, errors and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "va5ZPnwdpg0W"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "yhat = b + w * x_train_tensor\n",
    "error = yhat - y_train_tensor\n",
    "loss = (error ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERIK1XVWqemS"
   },
   "source": [
    "Now let's plot the **computation graph** for the **yhat** variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "colab_type": "code",
    "id": "eft1ShqTp8_k",
    "outputId": "7c169a26-e2d8-446f-d904-7453bc42c783"
   },
   "outputs": [],
   "source": [
    "make_dot(yhat, params={'b': b, 'w': w, 'yhat': yhat})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "We7c0NLkq07K"
   },
   "source": [
    "Let’s take a closer look at its components:\n",
    "\n",
    "* **blue boxes**: these boxes correspond to the **tensors** we use as **parameters**, the ones we’re asking PyTorch to **compute gradients** for;\n",
    "\n",
    "* **gray box**: a **Python operation** that involves a **gradient-computing tensor or its dependencies**;\n",
    "\n",
    "* **green box**: the tensor used as the **starting point for the computation** of gradients (assuming the **`backward()`** method is called from the **variable used to visualize** the graph)— they are computed from the **bottom-up** in a graph.\n",
    "\n",
    "Now, take a closer look at the **gray box** at the bottom of the graph: there are **two arrows** pointing to it, since it is **adding up two variables**, `b` and `w*x`. Seems obvious, right?\n",
    "\n",
    "Then, look at the **gray box** (`MulBackwardo0`) of the same graph: it is performing a **multiplication**, namely, `w*x`. But there is only **one arrow** pointing to it! The arrow comes from the **blue box** that corresponds to our parameter `w`.\n",
    "\n",
    "Why don’t we have a box for our **data x**? The answer is: **we do not compute gradients for it**! So, even though there are *more* tensors involved in the operations performed by the computation graph, it **only** shows **gradient-computing tensors and its dependencies**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGrlUBcMriPJ"
   },
   "source": [
    "Try using the `make_dot` method to plot the **computation graph** of other variables, like `error` or `loss`.\n",
    "\n",
    "The **only difference** between them and the first one is the number of **intermediate steps (gray boxes)**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "colab_type": "code",
    "id": "Z3T9goqOqIt3",
    "outputId": "5ca6ca7e-31d9-4ceb-8a0e-c417f1820c36"
   },
   "outputs": [],
   "source": [
    "make_dot(loss, params={'b': b, 'w': w, 'loss': loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VPZuuf-wt5Yk"
   },
   "source": [
    "What would happen to the computation graph if we set **`requires_grad`** to **`False`** for our parameter **`b`**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BnsSXOMctokE"
   },
   "outputs": [],
   "source": [
    "b_nograd = torch.randn(1, requires_grad=False, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "yhat = b_nograd + w * x_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "zzActDPeuLjb",
    "outputId": "b5345cf7-07c2-413e-e604-c2468c960d2a"
   },
   "outputs": [],
   "source": [
    "make_dot(yhat, params={'b': b_nograd, 'w': w, 'yhat': yhat})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fxojZtzpuQ6W"
   },
   "source": [
    "Unsurprisingly, the **blue box** corresponding to the **parameter a** is no more! \n",
    "\n",
    "Simple enough: **no gradients, no graph**.\n",
    "\n",
    "The **best thing** about the *dynamic computing graph* is the fact that you can make it **as complex as you want it**. You can even use *control flow statements* (e.g., if statements) to **control the flow of the gradients** (obviously!) :-)\n",
    "\n",
    "Let's build a nonsensical, yet complex, computation graph just to make a point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wddV-foIuOcO"
   },
   "outputs": [],
   "source": [
    "yhat = b + w * x_train_tensor\n",
    "error = yhat - y_train_tensor\n",
    "\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "if loss > 0:\n",
    "    yhat2 = w * x_train_tensor\n",
    "    error2 = y_train_tensor - yhat2\n",
    "\n",
    "loss += error2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "colab_type": "code",
    "id": "J6kP2dA-u6w4",
    "outputId": "986fbd2f-48dc-4918-c1db-22cf75d77728"
   },
   "outputs": [],
   "source": [
    "make_dot(loss, params={'b': b, 'w': w, 'loss': loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LB6C0xscvB3o"
   },
   "source": [
    "## Optimizer:  learning the parameters step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cwemv-t3vIbJ"
   },
   "source": [
    "So far, we’ve been **manually** updating the parameters using the computed gradients. That’s probably fine for *two parameters*… but what if we had a **whole lot of them**?! We use one of PyTorch’s **optimizers**, like [SGD](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) or [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam).\n",
    "\n",
    "---\n",
    "\n",
    "There are **many** optimizers, **SGD** is the most basic of them and **Adam** is one of the most popular. They achieve the same goal through, literally, **different paths**.\n",
    "\n",
    "---\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/PyTorch101_AI_Plus/main/images/sgd_adam_paths.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/PyTorch101_AI_Plus/main/images/sgd_nesterov_paths.png)\n",
    "\n",
    "In the code below, we create a *Stochastic Gradient Descent* (SGD) optimizer to update our parameters **b** and **w**.\n",
    "\n",
    "---\n",
    "\n",
    "Don’t be fooled by the **optimizer’s** name: if we use **all training data** at once for the update — as we are actually doing in the code — the optimizer is performing a **batch** gradient descent, despite of its name.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gOkDgwawBUR"
   },
   "outputs": [],
   "source": [
    "# Our parameters\n",
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Learning rate\n",
    "lr = 1e-1\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([b, w], lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lw8Sku2ev4ML"
   },
   "source": [
    "### step / zero_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i3ljYgxLv52K"
   },
   "source": [
    "An optimizer takes the **parameters** we want to update, the **learning rate** we want to use (and possibly many other hyper-parameters as well!) and **performs the updates** through its [**`step()`**](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step) method.\n",
    "\n",
    "Besides, we also don’t need to zero the gradients one by one anymore. We just invoke the optimizer’s [**`zero_grad()`**](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad) method and that’s it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YcNkxhfEvDlT",
    "outputId": "2d6ae35e-44d3-45b6-a89a-9f9c0202b006"
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1\n",
    "    yhat = b + w * x_train_tensor\n",
    "\n",
    "    # Step 2\n",
    "    error = yhat - y_train_tensor\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3\n",
    "    loss.backward()    \n",
    "    \n",
    "    # Step 4\n",
    "    # No more manual update!\n",
    "    # with torch.no_grad():\n",
    "    #     b -= lr * b.grad\n",
    "    #     w -= lr * w.grad\n",
    "    optimizer.step()\n",
    "    \n",
    "    # No more telling PyTorch to let gradients go!\n",
    "    # b.grad.zero_()\n",
    "    # w.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zHwVdA0HxCON"
   },
   "source": [
    "Cool! We’ve *optimized* the **optimization** process :-) What’s left?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAq8ytYfxK7w"
   },
   "source": [
    "## Loss: aggregating erros into a single value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h7f_eHQOxN5b"
   },
   "source": [
    "We now tackle the **loss computation**. As expected, PyTorch got us covered once again. There are many [loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) to choose from, depending on the task at hand. Since ours is a regression, we are using the [Mean Squared Error (MSE)](https://bit.ly/3hNYn4R) loss.\n",
    "\n",
    "---\n",
    "\n",
    "Notice that `nn.MSELoss` actually **creates a loss function** for us — **it is NOT the loss function itself**. Moreover, you can specify a **reduction method** to be applied, that is, **how do you want to aggregate the results for individual points** — you can average them (reduction=’mean’) or simply sum them up (reduction=’sum’).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eC4EH3rKxL1J",
    "outputId": "fad967b0-1bda-4dc2-8799-ba998d310fd7"
   },
   "outputs": [],
   "source": [
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Pq5dPX8zBZk4",
    "outputId": "cbfba17a-003e-4f46-a501-21a02334d564"
   },
   "outputs": [],
   "source": [
    "fake_labels = torch.tensor([1., 2., 3.])\n",
    "fake_preds = torch.tensor([1., 3., 5.])\n",
    "\n",
    "loss_fn(fake_preds, fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kcS4vSiTyG7U"
   },
   "source": [
    "We then **use** the created loss function to compute the loss given our **predictions** and our **labels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SxmHENASx3-A",
    "outputId": "ff4998c9-ba0c-4c7c-fc50-f806ae1b5519"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.SGD([b, w], lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1\n",
    "    yhat = b + w * x_train_tensor\n",
    "    \n",
    "    # Step 2\n",
    "    # No more manual loss!\n",
    "    # error = y_tensor - yhat\n",
    "    # loss = (error ** 2).mean()\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "\n",
    "    # Step 3\n",
    "    loss.backward() \n",
    "\n",
    "    # Step 4\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ey-gt8k4yUX6"
   },
   "source": [
    "At this point, there’s only one piece of code left to change: the **predictions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz('#./quiz/quiz3.b64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
